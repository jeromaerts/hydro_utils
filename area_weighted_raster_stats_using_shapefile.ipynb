{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350bf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import iris\n",
    "import iris.pandas\n",
    "import numpy as np\n",
    "from esmvalcore import preprocessor\n",
    "from iris.coords import DimCoord\n",
    "from iris.cube import Cube\n",
    "from pathos.threading import ThreadPool as Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b0d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Paths (Shown are for Snellius HPC)\n",
    "SHAPE_DIR = Path(\"/gpfs/home6/jaerts/fransje_code/catchment_shapefiles/\")\n",
    "NC4_DIR = Path(\"/gpfs/home6/jaerts/fransje_code/GSWP3-data/\")\n",
    "OUT_DIR = Path(\"/gpfs/home6/jaerts/fransje_code/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166cad92-1269-4cb2-9174-0baf9ec0118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regridding_target_cube(catchment_shapefile, spatial_resolution, buffer=1):\n",
    "    catchment_bounds = gpd.read_file(catchment_shapefile).bounds\n",
    "\n",
    "    buffer = 1\n",
    "    minx = int(catchment_bounds.minx.values[0]) - buffer\n",
    "    maxx = int(catchment_bounds.maxx.values[0]) + buffer\n",
    "    miny = int(catchment_bounds.miny.values[0]) - buffer\n",
    "    maxy = int(catchment_bounds.maxy.values[0]) + buffer\n",
    "\n",
    "    latitude = DimCoord(\n",
    "        np.linspace(\n",
    "            miny,\n",
    "            maxy,\n",
    "            int(np.divide((abs(miny) - abs(maxy)), spatial_resolution)),\n",
    "            dtype=float,\n",
    "        ),\n",
    "        standard_name=\"latitude\",\n",
    "        units=\"degrees\",\n",
    "    )\n",
    "    latitude.guess_bounds()\n",
    "    \n",
    "    longitude = DimCoord(\n",
    "        np.linspace(\n",
    "            minx,\n",
    "            maxx,\n",
    "            int(np.divide((abs(minx) - abs(maxx)), spatial_resolution)),\n",
    "            dtype=float,\n",
    "        ),\n",
    "        standard_name=\"longitude\",\n",
    "        units=\"degrees\",\n",
    "    )\n",
    "    longitude.guess_bounds()\n",
    "    \n",
    "    target_cube = Cube(\n",
    "        np.zeros((len(latitude.points), len(longitude.points)), np.float32),\n",
    "        dim_coords_and_dims=[(latitude, 0), (longitude, 1)],\n",
    "    )\n",
    "\n",
    "    return target_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648b7809-70c8-42e6-9712-e86d7dc3ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_weighted_shapefile_rasterstats(\n",
    "    catchment_shapefile,\n",
    "    catchment_netcdf,\n",
    "    statistical_operator,\n",
    "    output_dir,\n",
    "    output_csv=True,\n",
    "    return_cube=False,\n",
    "    regrid_first=True,\n",
    "    grid_resolution=0.1\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate area weighted zonal statistics of netcdfs using a shapefile to extract netcdf data.\n",
    "\n",
    "    catchment_shapefile:  str, catchment shapefile\n",
    "    catchment_netcdf:     str, netcdf file\n",
    "    statistical_operator: str, (mean, median (NOT area weighted), sum, variance, min, max, rms)\n",
    "    - https://docs.esmvaltool.org/projects/esmvalcore/en/latest/api/esmvalcore.preprocessor.html#esmvalcore.preprocessor.area_statistics\n",
    "    output_csv:          bool, True stores csv output and False stores netcdf output\n",
    "    regrid_first:        bool, True regrid cube first before extracting shape, False do not regrid first\n",
    "    grid_resolution:    float, grid cell size of target cube in degrees\n",
    "    Returns: iris cube, stores .csv file or .nc file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load iris cube of netcdf\n",
    "    cube = iris.load_cube(catchment_netcdf)\n",
    "    cube.dim_coords[1].guess_bounds()\n",
    "    cube.dim_coords[2].guess_bounds()\n",
    "    \n",
    "    # Create target grid and regrid cube\n",
    "    if regrid_first is True:\n",
    "        target_cube = regridding_target_cube(\n",
    "            catchment_shapefile, grid_resolution, buffer=1\n",
    "        )\n",
    "  \n",
    "        cube = preprocessor.regrid(cube, target_cube, scheme=\"area_weighted\")\n",
    "\n",
    "    # From cube extract shapefile shape\n",
    "    cube = preprocessor.extract_shape(\n",
    "        cube, catchment_shapefile, method=\"representative\"\n",
    "    )\n",
    "\n",
    "    # Calculate area weighted statistics\n",
    "    cube_stats = preprocessor.area_statistics(cube, statistical_operator)\n",
    "\n",
    "    if output_csv is True:\n",
    "        # Convert cube to dataframe\n",
    "        df = iris.pandas.as_data_frame(cube_stats)\n",
    "\n",
    "        # Change column names\n",
    "        df = df.reset_index()\n",
    "        df = df.set_axis([\"time\", cube_stats.name()], axis=1)\n",
    "\n",
    "        # Write csv as output\n",
    "        df.to_csv(\n",
    "            f\"{output_dir}/{Path(catchment_shapefile).name.split('.')[0]}_{catchment_netcdf.split('/')[-1].split('_')[0]}_{statistical_operator}.csv\"\n",
    "        )\n",
    "    else:\n",
    "        iris.save(\n",
    "            cube_stats,\n",
    "            f\"{output_dir}/{Path(catchment_shapefile).name.split('.')[0]}_{catchment_netcdf.split('/')[-1].split('_')[0]}_{statistical_operator}.nc\",\n",
    "        )\n",
    "\n",
    "    if return_cube == True:\n",
    "        return cube\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56951c22-fcca-44b5-a286-8010ff88cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_function_parallel(\n",
    "    shapefile_list=list,\n",
    "    netcdf_list=list,\n",
    "    operator_list=list,\n",
    "    output_dir_list=list,\n",
    "    threads=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs function area_weighted_shapefile_rasterstats in parallel.\n",
    "\n",
    "    shapefile_list:  str, list, list of input catchment shapefiles\n",
    "    netcdf_list:     str, list, list of input netcdf files\n",
    "    operator_list:   str, list, list of statistical operators (single operator)\n",
    "    output_dir_list: str, list, list of output directories\n",
    "    threads:         int,       number of threads (cores), when set to None use all available threads\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Set number of threads (cores) used for parallel run and map threads\n",
    "    if threads is None:\n",
    "        pool = Pool()\n",
    "    else:\n",
    "        pool = Pool(nodes=threads)\n",
    "    # Run parallel models\n",
    "    results = pool.map(\n",
    "        area_weighted_shapefile_rasterstats,\n",
    "        shapefile_list,\n",
    "        netcdf_list,\n",
    "        operator_list,\n",
    "        output_dir_list,\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7e09db-c921-4477-ab0a-235ac0d11b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_lists_for_parallel_function(NC4_DIR, SHAPE_DIR, OUT_DIR):\n",
    "    \"\"\"\n",
    "    This functions constructs list for running code in parallel.\n",
    "\n",
    "    NC4_DIR:              str, dir containing input netcdf files for area weighted statitic calculations\n",
    "    SHAPE_DIR:            str, dir containing catchment shapefiles\n",
    "    OUT_DIR:              str, dir for output storage\n",
    "\n",
    "    Returns: list of netcdfs, shapefiles, output directories\n",
    "    \"\"\"\n",
    "    netcdfs = glob.glob(f\"{NC4_DIR}/*nc\")\n",
    "    shapefiles = glob.glob(f\"{SHAPE_DIR}/*shp\")\n",
    "\n",
    "    output_dir = [OUT_DIR]\n",
    "\n",
    "    shapefile_list = shapefiles * len(netcdfs)\n",
    "    shapefile_list.sort()\n",
    "    netcdf_list = netcdfs * len(shapefiles)\n",
    "    output_dir_list = output_dir * len(shapefile_list)\n",
    "\n",
    "    operator_list = []\n",
    "\n",
    "    for netcdf in netcdf_list:\n",
    "        if \"tas\" in netcdf:\n",
    "            operator_list.append(\"mean\")\n",
    "        else:\n",
    "            operator_list.append(\"mean\")\n",
    "\n",
    "    return shapefile_list, netcdf_list, operator_list, output_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f178c3-450f-4584-876b-487dbc8bbc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct lists for parallel run\n",
    "(\n",
    "    shapefile_list,\n",
    "    netcdf_list,\n",
    "    operator_list,\n",
    "    output_dir_list,\n",
    ") = construct_lists_for_parallel_function(NC4_DIR, SHAPE_DIR, OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf286115-13af-45d7-a47e-0bf6894cdb16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 22s, sys: 38.9 s, total: 9min 1s\n",
      "Wall time: 9min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Test speed of parallel run\n",
    "run_function_parallel(shapefile_list, netcdf_list, operator_list, output_dir_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
